{
 "cells": [
  {
   "cell_type": "code",
   "id": "3cb1cc2f-2059-492d-8218-ccc66e976d92",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-05-15T19:54:12.822284Z",
     "start_time": "2025-05-15T19:54:12.804965Z"
    }
   },
   "source": [
    "from data_gen import generate_random_shapes_image, blur_image\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.amp\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "sigma_range=(0, 10)\n",
    "sigma_mean = (sigma_range[1] - sigma_range[0]) / 2\n",
    "sigma_std = (sigma_range[1] - sigma_range[0]) / 2\n",
    "# Custom Dataset\n",
    "class BlurDataset(Dataset):\n",
    "    def __init__(self,  length=10000):\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = generate_random_shapes_image()\n",
    "\n",
    "        # Convert to numpy for OpenCV blur\n",
    "        image_np = np.asarray(image)\n",
    "\n",
    "        # Generate random sigma and apply Gaussian blur\n",
    "        sigma = np.random.uniform(sigma_range[0], sigma_range[1])\n",
    "        blurred = blur_image(image_np, sigma)\n",
    "        blurred_pil = Image.fromarray(blurred)  # Convert back to PIL for transforms\n",
    "\n",
    "        blurred_tensor = transform(blurred_pil)\n",
    "\n",
    "        if blurred_tensor.shape != (1, 128, 128):\n",
    "            raise ValueError(f\"Unexpected tensor shape: {blurred_tensor.shape}\")\n",
    "\n",
    "        # Normalize sigma to be between 0 and 1\n",
    "\n",
    "        sigma_normalized = (sigma - sigma_mean) / sigma_std\n",
    "        return blurred_tensor, torch.tensor(sigma_normalized, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# CNN Model\n",
    "class BlurRegressionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlurRegressionCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 512),  # Corrected for 128x128 input\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.regressor(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    model_path = \"best_blur_model.pth\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, sigmas in tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"\n",
    "        ):\n",
    "            images, sigmas = images.to(device), sigmas.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, sigmas)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, sigmas in val_loader:\n",
    "                images, sigmas = images.to(device), sigmas.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, sigmas)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Saved best model with Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    batch_size = 128\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-3\n",
    "    val_split = 0.2\n",
    "\n",
    "    # Dataset\n",
    "    dataset = BlurDataset(length=12800)\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_size = int((1 - val_split) * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    global model\n",
    "    # Model\n",
    "    model = BlurRegressionCNN().to(device)\n",
    "\n",
    "    # Train\n",
    "    model = train_model(model, train_loader, val_loader, num_epochs, learning_rate)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "16f2abed2c2f1da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:54:21.964897Z",
     "start_time": "2025-05-15T19:54:17.251538Z"
    }
   },
   "source": [
    "main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  15%|█▌        | 12/80 [00:04<00:24,  2.78it/s]\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: 'zmq.backend.cython._zmq.Frame.__del__'\n",
      "Traceback (most recent call last):\n",
      "  File \"_zmq.py\", line 160, in zmq.backend.cython._zmq._check_rc\n",
      "  File \"/home/romain/PycharmProjects/MarketAnalysis/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_10680/1759737772.py\", line 1, in <module>\n",
      "    main()\n",
      "  File \"/tmp/ipykernel_10680/2813317593.py\", line 189, in main\n",
      "    model = train_model(model, train_loader, val_loader, num_epochs, learning_rate)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10680/2813317593.py\", line 118, in train_model\n",
      "    scaler.step(optimizer)\n",
      "  File \"/home/romain/PycharmProjects/MarketAnalysis/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py\", line 461, in step\n",
      "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/romain/PycharmProjects/MarketAnalysis/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py\", line 355, in _maybe_opt_step\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/romain/PycharmProjects/MarketAnalysis/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py\", line 355, in <genexpr>\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "               ^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"_zmq.py\", line 160, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7c8db23e1430>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/romain/PycharmProjects/MarketAnalysis/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 189\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    186\u001B[0m model \u001B[38;5;241m=\u001B[39m BlurRegressionCNN()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[0;32m--> 189\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining completed!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[14], line 118\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, num_epochs, lr)\u001B[0m\n\u001B[1;32m    115\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, sigmas)\n\u001B[1;32m    117\u001B[0m scaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m--> 118\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m    121\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/MarketAnalysis/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:461\u001B[0m, in \u001B[0;36mGradScaler.step\u001B[0;34m(self, optimizer, *args, **kwargs)\u001B[0m\n\u001B[1;32m    455\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munscale_(optimizer)\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28mlen\u001B[39m(optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    459\u001B[0m ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo inf checks were recorded for this optimizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 461\u001B[0m retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_opt_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    463\u001B[0m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstage\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m OptState\u001B[38;5;241m.\u001B[39mSTEPPED\n\u001B[1;32m    465\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "File \u001B[0;32m~/PycharmProjects/MarketAnalysis/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:355\u001B[0m, in \u001B[0;36mGradScaler._maybe_opt_step\u001B[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_maybe_opt_step\u001B[39m(\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    349\u001B[0m     optimizer: torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mOptimizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    352\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    353\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m    354\u001B[0m     retval: Optional[\u001B[38;5;28mfloat\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moptimizer_state\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf_per_device\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    356\u001B[0m         retval \u001B[38;5;241m=\u001B[39m optimizer\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "File \u001B[0;32m~/PycharmProjects/MarketAnalysis/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:355\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_maybe_opt_step\u001B[39m(\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    349\u001B[0m     optimizer: torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mOptimizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    352\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    353\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m    354\u001B[0m     retval: Optional[\u001B[38;5;28mfloat\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28msum\u001B[39m(\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    356\u001B[0m         retval \u001B[38;5;241m=\u001B[39m optimizer\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "eadfcd14ab426dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:02.975622Z",
     "start_time": "2025-05-15T19:53:02.960890Z"
    }
   },
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, sigmas in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, sigmas = images.to(device), sigmas.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, sigmas)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # The normalization in BlurDataset is: sigma_normalized = (sigma - sigma_min) / (sigma_max - sigma_min)\n",
    "            # So, to denormalize: sigma = sigma_normalized * (sigma_max - sigma_min) + sigma_min\n",
    "            outputs_denorm = outputs.cpu().numpy() * sigma_std + sigma_mean\n",
    "            sigmas_denorm = sigmas.cpu().numpy() * sigma_std + sigma_mean\n",
    "\n",
    "            predictions.extend(outputs_denorm)\n",
    "            ground_truths.extend(sigmas_denorm)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    predictions = np.array(predictions)\n",
    "    ground_truths = np.array(ground_truths)\n",
    "\n",
    "    return test_loss, predictions, ground_truths\n",
    "\n",
    "\n",
    "def plot_results(test_loss, predictions, ground_truths, save_dir=\"plots\"):\n",
    "    # Créer le répertoire pour sauvegarder les plots\n",
    "    Path(save_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # 1. Scatter plot : Prédictions vs Valeurs réelles\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(ground_truths, predictions, alpha=0.5, s=10)\n",
    "    plt.plot([0, 10], [0, 10], 'r--', label='Ligne idéale')\n",
    "    plt.xlabel('Sigma réel')\n",
    "    plt.ylabel('Sigma prédit')\n",
    "    plt.title(f'Prédictions vs Valeurs réelles (Test Loss: {test_loss:.4f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(Path(save_dir) / 'predictions_vs_ground_truth.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # 2. Histogramme des erreurs\n",
    "    errors = predictions - ground_truths\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(errors, bins=50, kde=True)\n",
    "    plt.xlabel('Erreur (Prédit - Réel)')\n",
    "    plt.ylabel('Fréquence')\n",
    "    plt.title('Distribution des erreurs de prédiction')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(Path(save_dir) / 'error_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # 3. Erreur absolue en fonction de sigma réel\n",
    "    abs_errors = np.abs(errors)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(ground_truths, abs_errors, alpha=0.5, s=10)\n",
    "    plt.xlabel('Sigma réel')\n",
    "    plt.ylabel('Erreur absolue')\n",
    "    plt.title('Erreur absolue en fonction de Sigma réel')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(Path(save_dir) / 'absolute_error_vs_ground_truth.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Boxplot des erreurs par plage de sigma\n",
    "    sigma_bins = np.linspace(0, 10, 11)\n",
    "    digitized = np.digitize(ground_truths, sigma_bins)\n",
    "    binned_errors = [abs_errors[digitized == i] for i in range(1, len(sigma_bins))]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(binned_errors, labels=[f'{sigma_bins[i]:.1f}-{sigma_bins[i+1]:.1f}'\n",
    "                                     for i in range(len(sigma_bins)-1)])\n",
    "    plt.xlabel('Plage de Sigma réel')\n",
    "    plt.ylabel('Erreur absolue')\n",
    "    plt.title('Distribution des erreurs par plage de Sigma')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(save_dir) / 'error_by_sigma_range.png')\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "6985f26ea8078f2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:09.621132Z",
     "start_time": "2025-05-15T19:53:06.003993Z"
    }
   },
   "source": [
    "    # Hyperparamètres\n",
    "    batch_size = 128\n",
    "    sigma_range = (0, 10)\n",
    "    test_size = 2000\n",
    "\n",
    "    # Charger le modèle entraîné\n",
    "    # model = BlurRegressionCNN().to(device)\n",
    "    global model\n",
    "    # model_path = \"best_blur_model.pth\"\n",
    "    # if not Path(model_path).exists():\n",
    "    #     raise FileNotFoundError(f\"Modèle non trouvé à {model_path}\")\n",
    "    # model.load_state_dict(torch.load(model_path))\n",
    "    # print(f\"Modèle chargé depuis {model_path}\")\n",
    "\n",
    "    # Créer le dataset de test\n",
    "    test_dataset = BlurDataset(length=test_size)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Critère de perte\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Évaluer le modèle\n",
    "    test_loss, predictions, ground_truths = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"MAE: {np.mean(np.abs(predictions - ground_truths)):.4f}\")\n",
    "    print(f\"RMSE: {np.sqrt(np.mean((predictions - ground_truths)**2)):.4f}\")\n",
    "\n",
    "    # Visualiser les résultats\n",
    "    plot_results(test_loss, predictions, ground_truths)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 16/16 [00:03<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2436\n",
      "MAE: 2.0838\n",
      "RMSE: 2.4679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10680/2777009023.py:75: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot(binned_errors, labels=[f'{sigma_bins[i]:.1f}-{sigma_bins[i+1]:.1f}'\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "acf4f38dcc622089"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
